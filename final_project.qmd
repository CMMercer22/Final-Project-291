---
title: "An Exploration of the Texas Education Agency 2022-2023 Accountability Ratings Dataset"
author: "Radiah Khan, Mercer Mercer, Nafisa Mohamed, Catherine Weeks"
format: pdf
editor: visual
bibliography: references.bib
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\LARGE\bfseries}
  - \posttitle{\end{center}}
---

\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE
)
```

```{r,include=FALSE}
library(broom)
library(kableExtra)
library(car)
```



# Introduction

Through this paper, we aim to address the question: Do schools in Texas with higher percentages of economically disadvantaged students have lower accountability ratings? To address this, we sourced our data from Texas’s annual academic accountability ratings, which it provides for all of its public and charter schools. This question is of particular interest now because the Academic accountability ratings are the benchmark the Texas Education Agency (TEA) uses to take over underperforming districts[@texaseducationagency2025]. This adds additional questions to what makes a school at risk for being taken over. The previous two districts to be taken over, Houston ISD and Fort Worth ISD, both had a large percentage of economically disadvantaged students. Some districts have begun redrawing their boundaries to avoid government takeovers[@edison2025]. If schools that predominantly serve economically disadvantaged students are closing to avoid a government takeover, this would cause an undue burden on those students, who would then have to travel further to get to classes, which can already be a barrier to education. Texas has also recently updated the criteria in its accountability rating system, which some teachers and superintendents claim has made earning a passing grade more difficult, potentially affecting whether student economic status influences academic ratings. Previous research has suggested a connection between a student's economic status and school performance. For example, a study from Stanford examining all public school districts in the United States found a strong relationship between district socioeconomic status and average academic achievement [@seanf.reardon2016]. Another study found a strong connection between student socioeconomic status and student achievement[@jenniferelizabethbarry2006]. But neither of these are recent or uses the data that Texas uses to internally select what it considers to be failing schools. Our paper aims to provide additional insight into how Texas’s academic achievement ratings relate to students' socioeconomic status, given the state’s new academic achievement rating system. Texas’s academic accountability rating system reports multiple variables containing information on school performance, but the ones of interest to us are as follows: Academic Growth Score, Overall Score, School Progress Score, and multiple variables indicating distinction in particular areas, which we have combined into a single variable that tracks total earned distinctions.

```{r,include=FALSE}
library(readr)
School_Year_2022_2023_Statewide_Accountability_Ratings_20251119 <- read_csv("School_Year_2022-2023_Statewide_Accountability_Ratings_20251119.csv")
#View(School_Year_2022_2023_Statewide_Accountability_Ratings_20251119)
```

```{r,include=FALSE}
library(janitor)
cleaned_name_school <- clean_names(School_Year_2022_2023_Statewide_Accountability_Ratings_20251119)
```

```{r,include=FALSE}
colnames(cleaned_name_school)
```

```{r,include=FALSE}
#Economically disadvantaged grouped by regions 
library(tidyverse)
data <-cleaned_name_school |>
  mutate(percent_economically_disadvantaged = 
           as.numeric(str_replace_all(percent_economically_disadvantaged, "[^0-9.]", "")))
```

```{r,echo=FALSE}
#changed to be highest avg
plot_economical_region <- data |>
 select(district , percent_economically_disadvantaged)|>
  group_by(district) |>
  summarise(avg_disadvantaged = mean(percent_economically_disadvantaged, na.rm = TRUE)) |>
  arrange(desc(avg_disadvantaged))
plot_economical_region
```

```{r,include=FALSE}
highest_10_region <- plot_economical_region |> head(10)
```

## Plot 1

```{r,echo=FALSE}
#plotting the data 
#change interpretation of the plot to be about highest 10. Mention location, type, overall scores for these.
ggplot(highest_10_region, aes(x = reorder(district, avg_disadvantaged), 
                                   y = avg_disadvantaged, 
                                   fill = avg_disadvantaged)) +
  geom_col() +
  coord_flip() +
   scale_fill_gradient(
    low = "deepskyblue",   
    high = "springgreen3"  
  ) +
  labs(
    title = "Districts with the Highest Average Economically Disadvantaged Student Demographic (Top 10)",
    x = "District",
    y = "Economically Disadvantaged (%)",
    fill = "Disadvantaged %"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
      panel.grid.major=  element_blank(),
      panel.grid.minor = element_blank())
  
      
```

The horizontal bar chart of the ten districts with the lowest average percent economically disadvantaged highlights a small group of districts and charter schools with very low concentrations of economically disadvantaged students. Brock ISD and Trivium Academy stand out at the top of this list, followed by Leadership Prep School and a mix of charter and small district programs; the x-axis values show these averages are near or below single-digit percentages. This pattern suggests these districts are outliers in the dataset, and because they are small or specialized (charter and specialty schools), their demographic profiles differ substantially from the statewide mix — an important caveat when comparing demographic measures to performance outcomes.

## Plot 2 "Overall Rating"



```{r,echo=FALSE}
ggplot(data, aes(x = overall_rating, fill = overall_rating)) +
  geom_bar() +
  scale_fill_brewer(palette = "YlGnBu") + 
  theme_minimal() +
  labs(
    title = "Count of Districts by Rating Categories",
    x = "Rating Category",
    y = "Number of Districts",
    fill = "Overall Rating"
  )+
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
      panel.grid.major=  element_blank(),
      panel.grid.minor = element_blank())


```

The rating-distribution bar chart shows most districts receive mid-to-high ratings: B and C are the most common categories (with B the largest, followed by C and A), while D, F, and Not Rated are substantially less frequent. In other words, most districts fall into the mid-to-high rating range, creating a central peak with short tails at both the high and low ends. This shape means the distribution is clustered rather than widely spread, so many districts have similar overall ratings and only a minority sit at the extremes. Practically, that implies comparisons based on letter grades will group many districts together and finer distinctions will rely on the underlying numeric scores or other measures to separate performance within that large central mass.

## Plot 3 `Academic Growth Score`

```{r,echo=FALSE}
#Academic growth score and overall rating
data <- data |>
  mutate(
    academic_growth_score = as.numeric(str_replace_all(academic_growth_score, "[^0-9.]", "")),
    overall_score = as.numeric(str_replace_all(overall_score, "[^0-9.]", ""))
  )

ggplot(data, aes(x = academic_growth_score, y = overall_score)) +
  geom_point(position = "jitter", na.rm = TRUE) +
  theme_minimal() +
  labs(
    title = "Academic Growth and Overall Rating",
    x = "Academic Growth Score",
    y = "Overall Score"
  ) +
  #scale_x_continuous() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  )
```

The scatterplot of Academic Growth Score versus Overall Score reveals a clear positive relationship: higher academic growth scores tend to accompany higher overall scores, with many schools clustered along an upward trend from roughly 50–60 overall at lower growth values to 85–100 overall at higher growth values. The spread is wider at lower growth levels and tighter near the top, suggesting a ceiling effect where very high growth scores correspond to consistently high overall scores; this reinforces that academic growth is an important predictor of overall performance, although variability at lower growth levels indicates other factors also influence overall score.



```{r,include=FALSE}
filtered_df <- data |>
  filter(grades_served == "09 - 12")
filtered_df
```

```{r,include=FALSE}
filtered_df_pivoted <- filtered_df |>
  pivot_longer(
    cols = starts_with("distinction_"),
    values_to = "distinction_status",
    names_to  = "Distinction Type"
  ) 
filtered_df_pivoted
```

```{r,include=FALSE}
colnames(filtered_df_pivoted)
```

```{r,include=FALSE}
final_df <- filtered_df_pivoted |>
  group_by(campus, district) |>
  summarise(
    number_of_students = first(number_of_students), #to get the unique values
    academic_growth_score = first(academic_growth_score),
    percent_economically_disadvantaged = first(percent_economically_disadvantaged),
    overall_score = first(overall_score),
    school_progress_score = first(school_progress_score),
    distinction_score = sum(distinction_status  == "Earned", na.rm = TRUE),
    .groups = "drop"
  )
final_df

```

\newpage 

# Methods

#### Data Source

The data set used in this report is from the Texas Education Agency’s 2025 State, Region, District, and Campus-Level Accountability Report. The data in this report originates from public and charter schools in Texas overseen by the Texas Education Agency (TEA), who are required to submit all student standardized tests and other academic performance data [@texaseducationagency2025]. Each observation in this data set represents a different campus.

Before building our model, we performed some data pre-processing steps to clean and organize the initial data set for analysis:

### Dataset Processing Steps 

1.  The dataframe had column names that were either capitalized inconsistently/had multiple spaces in between words; which made parsing the names harder for R to work on. So we cleaned the column names. (e.g., `School Progress Score` $\rightarrow$ `school_progress_score`)

2.  The data was filtered to grades 09-12 using `grades_served` column to keep our results consistent and specific to only high school students.

3.  There were 7 columns of distinction for each school (Distinction ELA/Reading, Distinction Mathematics, Distinction Science, Distinction Soc Studies,Distinction Progress, DIstinction Closing the Gaps, Distinction PostSecondary Readiness). All of them had values of `Earned`, `Not Earned`, `NA`. We reshaped this wide data into long data and merged it into one single column named `distinction_status`.

4.  However, after creating the distinction_status column, each school had seven rows. To prevent this from creating inconsistencies in the results of the chosen model later, we created an additional column, `distinction_score`, which counts the number of times each school earned distinction out of seven and assigns a single score to each school.

5.  We removed all the rows with NA values.

6.  While working on this data set, the numeric variables had their values as characters and not numeric. So we converted them to numeric values for building our model.

The final variables selected for analysis are:

-   `number_of_students`
-   `academic_growth_score`
-   `percent_economically_disadvantaged`
-   `overall_score`
-   `school_progress_score`
-   `distinction_score`
-   `campus`
-   `district`

<center>

#### Data Observations Flow

**Initial dataset:** 10,253 observations

↓

**Filtered to grades 9–12:** 1,394 observations

↓

**After NA removal:** 1,356 observations

</center>


#### Variable Units and Range

The original data contains many variables related to school performance, but in this paper we will focus on schools’ Overall Score, their Academic Growth Scores, their Distinction Scores, the number of students, and the percentage of students who are economically disadvantaged. Before selecting our model, we chose one variable from here as response variable and the rest as explanatory variable.

Table 1: Response Variable Information

| Variable      | Type    | Range        |
|---------------|---------|--------------|
| Overall Score | Numeric | 0–100 points |

Table 2: Explanatory Variable Information

| Variable                           | Type    | Unit of Observation | Range   |
|------------------------------------|---------|---------------------|---------|
| Number of students                 | Numeric | Students            | 1–5,317 |
| Academic growth score              | Numeric | School              | 0–100   |
| Percent economically disadvantaged | Numeric | School              | 0–100%  |
| School progress score              | Numeric | School              | 0–100   |
| Distinction score                  | Numeric | School              | 0–7     |

##### Variable Descriptions

1.  `Overall Score` : This variable represents the school’s final accountability rating on a 0-100 scale. It represents a schoolwide summary of academic performance, progress, and equity. This score is calculated by using three accountability domains: Student Achievement, School Progress, and Closing the Gaps. For this score, 70% is made up of either the Student Achievement or School progress (it picks the “better outcome”) and 30% of it is from their Closing the Gaps domain score (which evaluates how well a school is supporting the performance of specific students, specifically economically disadvantaged students, students from certain racial groups, and special needs students among others). The data handbook further explains how these were specifically calculated [@texaseducationagency2025].

2.  `Academic Growth Score` : This variable is a measure of how much students improved academically over time. It is calculated using a raw growth score which is based on whether students met, exceeded, or fell short of STAAR progress (which is further defined in the data handbook). This raw growth score is then converted to a scaled score using a table, which then becomes the academic growth score, which is also measured on a 0-100 scale.

3.  `Distinction Score` : This variable is derived from the 7 categories of Distinction that schools report. We explained this derivation in our dataset processing steps. But first here is what distinction means across the categories: It is calculated by comparing a school’s performance on multiple indicators to a group of similar campuses. If it places in the top quartile on at least 50% of indicators (for elementary and middle school) or 33% (for high school), it received the distinction. This means that the distinction score is whether or not the school performed exceptionally well compared to similar schools, and is a binary value of 0 (no distinction) or 1 (distinction) across multiple categories. So the distinction score variable sums up the amount of category a school got distinction and reports it on a 0-7 scale.

4.  `Percent economically disadvantaged` : The percentage of students who are economically disadvantaged represents the percentage of students who are economically disadvantaged according to a few criteria. These include eligibility for free and reduced lunch prices and SNAP/TANF participation.

5.  `Number of students` : This variable represents the number of students enrolled in each campus.

6.  `School Progress Score` :

#### Model Specification

We built a multiple linear regression model using nested F test model.

##### Confitoo

```{r,include=FALSE}
final_df$academic_growth_score <- as.numeric(final_df$academic_growth_score)
final_df$distinction_score <- as.numeric(final_df$distinction_score)
final_df$school_progress_score<- as.numeric(final_df$school_progress_score)
final_df$percent_economically_disadvantaged<- as.numeric(final_df$percent_economically_disadvantaged)
final_df$number_of_students<- as.numeric(final_df$number_of_students)
```


The reduced model:
$$
\widehat{\text{overall score}}_i =
\beta_0 +
\beta_1 \cdot \text{academic growth score}_i +
\beta_2 \cdot \text{distinction score}_i +
\beta_3 \cdot \text{school progress score}_i 
$$

```{r,echo=FALSE}
reduced_model <- lm(
  overall_score ~ academic_growth_score +
    distinction_score +
    school_progress_score,
  data = final_df
)

broom::tidy(reduced_model) |>
  kableExtra::kable()
```

The full model:

$$
\widehat{\text{overall score}}_i
 = \beta_0 + \beta_1 \cdot \text{academic growth score}_i + \beta_2 \cdot \text{distinction score}_i + \beta_3 \cdot \text{school progress score}_i$$ $$+ \beta_4 \cdot \text{percent economically disadvantaged}_i + \beta_5 \cdot \text{number of students}_i
$$

```{r,echo=FALSE}
full_model <- 
lm(overall_score ~ academic_growth_score+distinction_score+ school_progress_score + percent_economically_disadvantaged+number_of_students, data = final_df )
broom::tidy(full_model) |>
  kableExtra::kable()
```

#### Evaluating assumptions of the model 

The four assumptions we need to evaluate are : Linearity, Independence, Normality, Equal Variance. 

Assuming independence was maintained, we check the other three conditions. However, we discussed more about the concerns of independence in the discussion section (@sec-discussions)

For checking the linearity and equal variance we check the residual plot of the reduced and full model. See the detailed plot in Appendix A. 

For both of the reduced and full model, it violates the linearity, equal variance and normality condition. From the residual plot, we see that it widens as the x gets bigger and there is a pattern above and below y= 0 line. There seems to be unusual points as well which we will investigate later. 

In the Q-Q plot, for the reduced model it the points deviates from the reference line. 



#### Model Selection

To determine the model, we started with calculating the VIF to detect any multicollinearity issues among the explanatory variables. We calculated this using both the reduced and full model. There were no multicollinearity issues given all of the VIFs show little collinearity < 5 (see Appendix B for detailed results).

We began to test our model using forward model selection with AIC. See Appendix C for detailed results. According to the results,the AIC drops when we add `number of students`, it further drops when we add `percent economically disadvantaged` as well.  


To find the influential points, we first filtered the data to find the schools with leverage that was outside of the acceptable range, then further filtered that list to the schools that additionally had a standardized residual that was outside of the range. After filtering, we were left with only five schools that were potential influential points. We refit the model without them and saw no noticeable difference, so we added them back into the final data set. See Appendix D for detailed results. 


# Discussion {sec-discussions}
The goal of this study was to address the question whether schools in Texas with higher percentages of economically disadvantaged students have lower accountability ratings. Some things should be considered before drawing any conclusions from this model. First, one potential issue with this model is a violation of independence among the variables. Because the Overall Score variable is calculated using some of the metrics within the explanatory variables, we have some concern about this model's independence. However, one of the strengths of this study was a very low VIC and AIC across all variables. So we are somewhat confident that this should not be a major problem, as it indicates the variables are not collinear. Another thing to consider is that this data is only from one year, and it is very difficult to build a dataset with longer term data due to Texas’s constant fluctuations in how it reports and calculates the same data across many years. Next, the major strength of this study is that the model performs very well and yields statistically significant results.  Finally, the conclusions drawn from this should also only be used to form conclusions about public and charter schools within Texas in the academic year 2024-2025. This is because the metrics used to create many of the variables tested are specific to Texas’s academic measurements, such as the STAAR test, and cannot easily be compared with those of other States, and the data used is limited to only Texas public and charter schools.  



\newpage 

# Appendix {.appendix}

## Appendix A: Conditions of the linear regression model {#sec-conditions}
We explored the residual and Q-Q plot of both of the model to see if it violates the linearity, nornmality and equal variance condition. 
```{r,echo=FALSE}
par(mfrow = c(1, 2))
plot(reduced_model, which = 1)
plot(reduced_model, which = 2)
mtext("Diagnostic Plots for the Reduced Model", 
      side = 3, line = -2, outer = TRUE, cex = 1.2, font = 2)
par(mfrow = c(1, 1))
```
```{r,echo=FALSE}
par(mfrow = c(1, 2))
plot(full_model, which = 1)
plot(full_model, which = 2)
mtext("Diagnostic Plots for the Full Model", 
      side = 3, line = -2, outer = TRUE, cex = 1.2, font = 2)
par(mfrow = c(1, 1))
```

## Appendix B: VIF {#sec-vif}
```{r,echo=FALSE}
vif(reduced_model)
vif(full_model)
```

## Appendix C: Stepwise AIC {#sec-aic}

```{r,echo=FALSE}
add1(reduced_model,
     scope = ~ number_of_students +
              academic_growth_score +
              percent_economically_disadvantaged +
              school_progress_score +
              distinction_score,
     test = "F")
```


## Appendix D: Influential Points {#sec-influential}
```{r}
influence <- augment(full_model)
influence <- influence |> mutate(row_id = row_number())

```
```{r}
k_plus_one <- length(coef(full_model))
n <- nrow(final_df)

# Filtering the data to view observations with high leverage
influence |> filter(.hat > 2 * k_plus_one/n)
leverage_index <- influence |> filter(.hat > 2 * k_plus_one/n) |> select(row_id) |> pull()
save <- final_df[leverage_index,]
augment_df<-influence[leverage_index,]

save$row_id <- leverage_index

augment_df<- augment_df |> left_join(save|>select(campus, district,row_id), join_by("row_id"=="row_id"))

```
```{r}
##std residuals
augment_df1<-augment_df |> filter(.std.resid < -2 | .std.resid > 2)
std_resid_id <- augment_df |> filter(.std.resid < -2 | .std.resid > 2) |> select(row_id) |> pull()
```
```{r,include=FALSE}
#removing the influential datapoints 
final_df <-final_df[-1130,]
final_df <-final_df[-892,]
final_df <-final_df[-389, ]
final_df <-final_df[-333, ]
```
```{r,include=FALSE}
#refitting the datapoints 
new_model <- lm(overall_score ~ academic_growth_score+distinction_score+ school_progress_score + percent_economically_disadvantaged+number_of_students, data = final_df )
summary(new_model)
```
```{r,include=FALSE}
#the refitted linear regression plots 
reduced_model_logged <-
  lm(overall_score ~ academic_growth_score+distinction_score+ school_progress_score, data = final_df )
summary(reduced_model_logged)
```
